{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "drivendata waterpump.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mspoorendonk/drivendata/blob/marc/drivendata_waterpump.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9Hngyu5E6R"
   },
   "source": [
    "# Analysis of condition of water points in Tanzania\n",
    "\n",
    "Problem statement:\n",
    "predict the operating condition of a waterpoint for each record in the dataset: functioning, functioning but needs repair, not functioning\n",
    "\n",
    "\n",
    "Approach\n",
    "1. Download datasets\n",
    "1. Explore data and understand which features are relevant for the prediction. \n",
    "1. Clean data [Bart]\n",
    "1. Engineer some derived features\n",
    "1. decide on a method for predicting (trees or neuralnets or knn or ...)\n",
    "1. perform a train / test / validate split on the data\n",
    "1. Train model on training values and labels\n",
    "1. Predict training labels that correspond to training values\n",
    "1. Report the accuracy\n",
    "1. Tune hyperparameters with gridsearch\n",
    "1. Predict the test labels\n",
    "1. Submit CSV [Marc]\n",
    "\n",
    "\n",
    "TODO:\n",
    "here: check xgboost, pandas, bokeh (interactief)\n",
    "somewhere else: how to deploy a model in production. What software and frameworks etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwqftAGR-k2J"
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PuCGSSOKu8-h",
    "outputId": "4e1ba0b1-c2e7-415d-aa7f-0f9f9b61db51",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    }
   },
   "source": [
    "# installations\n",
    "\n",
    "!pip install gmaps"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CJlE0N65YUFd",
    "outputId": "d185adbf-f040-41dc-fac3-18b6b92f5566",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    }
   },
   "source": [
    "# imports\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import gmaps\n",
    "import IPython\n",
    "from sklearn import tree # to create a decision tree\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics # to compute accuracy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import preprocessing # for normalizing data for knn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pydotplus # To create our Decision Tree Graph\n",
    "from IPython.display import Image  # To Display a image of our graph\n",
    "from IPython.display import display\n",
    "\n",
    "from ipywidgets.embed import embed_minimal_html\n",
    "\n",
    "# Seaborn visualization library\n",
    "import seaborn as sns # for pairplots\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOSkSXlq-gzD"
   },
   "source": [
    "# Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tGa2jMrp23Sm",
    "outputId": "7c82da7a-96fa-4ae3-9b26-f360fb799bd1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    }
   },
   "source": [
    "# download datasets from driven-data.org. Urls copied from data download section on website.\n",
    "# They expire after 2 days or so. Then you need to copy/paste them again.\n",
    "\n",
    "# testvalues\n",
    "!wget \"https://drivendata-prod.s3.amazonaws.com/data/7/public/702ddfc5-68cd-4d1d-a0de-f5f566f76d91.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARVBOBDCY3EFSLNZR%2F20200927%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200927T185304Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f2b7c554cb780a1facf849dc85cd18a0ce5110100690a748eaa1df42f43a12da\" -O test_values.csv\n",
    "# training labels\n",
    "!wget \"https://drivendata-prod.s3.amazonaws.com/data/7/public/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARVBOBDCY3EFSLNZR%2F20200927%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200927T185304Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=f91daa03811de5cb244f5f2d8446fb46a99eb37bedf7bd0c609d8b076bebfbe2\" -O training_labels.csv\n",
    "# training values\n",
    "!wget \"https://drivendata-prod.s3.amazonaws.com/data/7/public/4910797b-ee55-40a7-8668-10efd5c1b960.csv?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIARVBOBDCY3EFSLNZR%2F20200927%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200927T185304Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=b38e27dab8fac51df99d1ec837ffd2f4a3c3e1ffd48494951a846a144f88434f\" -O training_values.csv"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s5JdImi9qyql"
   },
   "source": [
    "# Boundary coordinates of Tanzania\n",
    "# Source: https://en.wikipedia.org/wiki/List_of_countries_by_northernmost_point (and similar)\n",
    "tanzania_lat = [-11.750-0.1, -0.983+0.1]\n",
    "tanzania_lon = [29.167-0.1, 40.250+0.1]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data location\n",
    "data_path = ''\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TlOL83NOOp9n",
    "outputId": "f1473e4e-a572-42ed-9413-b57797dcdc6d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    }
   },
   "source": [
    "# Load training values\n",
    "na_values = {\n",
    "#     'longitude': 0.0,\n",
    "#     'latitude':-2.e-8,\n",
    "    'gps_height': 0,\n",
    "#     'wpt_name': 'none',\n",
    "    'construction_year': 0,\n",
    "#     'population': 0,\n",
    "#     'district_code': 0,\n",
    "}\n",
    "# na_values = {}\n",
    "training_values = pd.read_csv(data_path + 'training_values.csv',\n",
    "                              parse_dates=['date_recorded'],\n",
    "                              index_col='id',\n",
    "                              na_values=na_values)\n",
    "# Drop column(s) without information\n",
    "training_values.drop(columns=['num_private'], inplace=True)\n",
    "print('Shape: ', training_values.shape)\n",
    "# Show example\n",
    "display(training_values.iloc[:5, 0:20])\n",
    "display(training_values.iloc[:5, 20:])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load training labels\n",
    "training_labels = pd.read_csv(data_path + 'training_labels.csv',\n",
    "                             index_col='id')\n",
    "training_labels.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Merge training values and -labels\n",
    "training_all = pd.merge(training_values, training_labels, on='id')\n",
    "training_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load test values\n",
    "test_values = pd.read_csv(data_path + 'test_values.csv', \n",
    "                          parse_dates=['date_recorded'],\n",
    "                          index_col='id',\n",
    "                          na_values=na_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e1LWiBzgPUyv"
   },
   "source": [
    "# column_names = ''\n",
    "# for n in training_values.columns:\n",
    "#     column_names = column_names + \"'\" + n + \"', \"\n",
    "# print(column_names) # print a string from which we can copy/paste the following lists\n",
    "\n",
    "columns_time = ['date_recorded']\n",
    "columns_numerical = ['amount_tsh', 'gps_height', 'longitude', 'latitude', 'population',\n",
    "             'construction_year', 'region_code', 'district_code']\n",
    "columns_categorical = ['funder', 'installer', 'wpt_name', 'basin', 'subvillage',\n",
    "               'region', 'lga', 'ward', 'public_meeting', 'recorded_by',\n",
    "               'scheme_management', 'scheme_name', 'permit', 'extraction_type', \n",
    "               'extraction_type_group', 'extraction_type_class', 'management', \n",
    "               'management_group', 'payment', 'payment_type', 'water_quality', \n",
    "               'quality_group', 'quantity', 'quantity_group', 'source', 'source_type', \n",
    "               'source_class', 'waterpoint_type', 'waterpoint_type_group']\n",
    "columns_location = ['latitude', 'longitude', 'gps_height', 'wpt_name', 'basin', 'subvillage',\n",
    "                    'region', 'region_code', 'district_code', 'lga', 'ward']\n",
    "\n",
    "print('Time: ', len(columns_time))\n",
    "print('Numerical: ', len(columns_numerical))\n",
    "print('Categorical: ', len(columns_categorical))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show main training data characteristics\n",
    "training_values.info()\n",
    "training_values.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Investigate duplicate rows\n",
    "headers = list(training_values)\n",
    "duplicate_full = training_values.duplicated(subset=headers, keep=False)\n",
    "duplicate_full_all = training_all.duplicated(subset=(headers + ['status_group']), keep=False)\n",
    "print('Number of fully duplicate rows: ', duplicate_full.sum())\n",
    "print('Number of fully duplicate rows (incl label): ', duplicate_full_all.sum())\n",
    "print('Examples of duplicate rows:')\n",
    "display(training_values[duplicate_full].sort_values(by=headers).head(10))\n",
    "\n",
    "# Find the rows that are duplicate apart from the label\n",
    "id_diff = set(training_values[duplicate_full].index).difference(training_all[duplicate_full_all].index)\n",
    "print('Duplicate apart from label:')\n",
    "display(training_all.loc[id_diff].head(10))\n",
    "\n",
    "# Mark duplicates for removal\n",
    "flag_droprows = training_values.sort_index().duplicated(subset=headers, keep='first')\n",
    "# Add both rows with different label\n",
    "flag_droprows[id_diff] = True\n",
    "print('Marked for removal: {}'.format(flag_droprows.sum()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Investigate Total static head (amount_tsh)\n",
    "# Note that the units are unknown\n",
    "fig, ax = plt.subplots()\n",
    "training_values['amount_tsh'].plot.hist(ax=ax, log=True, bins=20)\n",
    "n_zero_tsh= (training_values['amount_tsh']==0).sum()\n",
    "n_total = training_values.shape[0]\n",
    "print('Rows where amount_tsh == 0.0: {}, ({:3d}%)'\\\n",
    "      .format(n_zero_tsh,int(round(100*n_zero_tsh/n_total, 2))))\n",
    "# Conclusion: amount_tsh looks okay, some zeros might actually be missing\n",
    "# values, but cannot be distinguished from measured zeros."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Investigate date_recorded\n",
    "fig, ax = plt.subplots()\n",
    "training_values['date_recorded'].dt.year.plot.hist(ax=ax)\n",
    "display(training_values['date_recorded'].dt.year.value_counts(sort=False))\n",
    "ax.set_title('Histogram of date_recorded')\n",
    "year2011 = (training_values['date_recorded'].dt.year == 2011)\n",
    "year2012 = (training_values['date_recorded'].dt.year == 2012)\n",
    "year2013 = (training_values['date_recorded'].dt.year == 2013)\n",
    "fig, ax2 = plt.subplots()\n",
    "training_values[year2011]['date_recorded'].dt.month.plot.hist(ax=ax2, bins=12,\n",
    "    range=(1, 12), log=True)\n",
    "ax2.set_title('2011, records per month')\n",
    "fig, ax3 = plt.subplots()\n",
    "training_values[year2012]['date_recorded'].dt.month.plot.hist(ax=ax3, bins=12,\n",
    "    range=(1, 12), log=True)\n",
    "ax3.set_title('2012, records per month')\n",
    "fig, ax4 = plt.subplots()\n",
    "training_values[year2013]['date_recorded'].dt.month.plot.hist(ax=ax4, bins=12,\n",
    "    range=(1, 12), log=True)\n",
    "ax4.set_title('2013, records per month')\n",
    "# Conclusion: dates look OK."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Investigate GPS height\n",
    "# Note that GPS height is inaccurate, deviations of 120 m are not uncommon.\n",
    "height_neg = training_values['gps_height'] < 0\n",
    "height_pos = training_values['gps_height'] > 0\n",
    "height_zero = training_values['gps_height'] == 0\n",
    "fig, ax = plt.subplots()\n",
    "training_values['gps_height'].plot.hist(ax=ax)\n",
    "ax.set_title('Histogram of GPS Height')\n",
    "fig, ax = plt.subplots()\n",
    "training_values[height_neg]['gps_height'].plot.hist(ax=ax)\n",
    "ax.set_title('Histogram of GPS Height (strictly negative)')\n",
    "print('Rows with zero height: {}'.format(height_zero.sum()))\n",
    "print('Rows with negative height: {}'.format(height_neg.sum()))\n",
    "fig, ax = plt.subplots()\n",
    "training_values['gps_height'].plot.hist(ax=ax, range=(-20,20))\n",
    "print(training_values[~height_zero]['gps_height'].median())\n",
    "# Conclusion: GPS height looks OK. Negative values can be explained by\n",
    "# inaccuracy of measurement. Zeros are most likely missing values,\n",
    "# since they occur much more frequently than other values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Investigate region\n",
    "# Note: Tanzania has 31 regions, 169 districts (2012)\n",
    "# https://en.wikipedia.org/wiki/Districts_of_Tanzania\n",
    "region_counts = training_values[['region_code','region']]\\\n",
    "    .sort_values(by='region').value_counts(sort=False)\n",
    "display(region_counts)\n",
    "display(training_values.query('region_code==5 & region==\"Tanga\"')\\\n",
    "    .loc[:, columns_location].head(10))\n",
    "# Conclusion: some region codes (5, 11, 14, 17, 18) seem to refer to multiple\n",
    "# regions. However, if the region with the highest count is correct, this\n",
    "# affects only 123 rows. Some regions (Arusha, Lindi, Mtwara, Mwanza, Pwani,\n",
    "# Shinyanga, Tanga) have multiple region codes associated to them.\n",
    "# Assuming that the most common mapping is correct, this affects around 2500 rows.\n",
    "# Solutions:\n",
    "# - Use only latitude and longitude as location data. However, the region,\n",
    "#   district, lga, ward might contain information about governance.\n",
    "# - Remove dubious location data and/or mark as missing.\n",
    "# - Use an external source (e.g. GeoNames.org) to verify which values are\n",
    "#   most likely incorrect and replace those."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Investigate district\n",
    "# Note: according to wikipedia, regions have up to 10 districts\n",
    "display(training_values.sort_values(by='district_code')\\\n",
    "    .value_counts(subset='district_code',sort=False))\n",
    "display(training_values.sort_values(by='district_code')\\\n",
    "    .value_counts(subset=['region', 'district_code'],sort=False))\n",
    "# Investigate example of district_code > 10\n",
    "display(training_values.query('district_code==80')\\\n",
    "    .loc[:,['region','region_code','district_code','subvillage']].head(12))\n",
    "# Conclusion: all district codes larger than 10 are most probably duplicates of\n",
    "# other codes. This affects some 4200 rows. The tuple (region_code,\n",
    "# district_code) should uniquely identify a region, so we can derive a new\n",
    "# feature based on these codes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Investigate location (latitude, longitude)\n",
    "\n",
    "# Check if latitude, longitude is actually inside Tanzania (or NaN)\n",
    "lon_isna = training_values['longitude'].isna()\n",
    "lat_isna = training_values['latitude'].isna()\n",
    "lon_in_range = (tanzania_lon[0] <= training_values['longitude']) & \\\n",
    "               (training_values['longitude'] <= tanzania_lon[1])\n",
    "lat_in_range = (tanzania_lat[0] <= training_values['latitude']) & \\\n",
    "               (training_values['latitude'] <= tanzania_lat[1])\n",
    "pos_isna = lon_isna | lat_isna\n",
    "pos_in_range = lon_in_range & lat_in_range\n",
    "print('Number of missing (n/a) coordinates: ', pos_isna.sum())\n",
    "print('Number of invalid coordinates: ', (~pos_in_range & ~pos_isna).sum())\n",
    "\n",
    "# Investigate duplicate locations\n",
    "duplicate_location = training_values.duplicated(\n",
    "    subset=['longitude', 'latitude'], keep=False)\n",
    "print('Number of rows with duplicate locations: ',\n",
    "    (duplicate_location & ~pos_isna & ~duplicate_full).sum())\n",
    "training_values[duplicate_location & ~pos_isna & ~duplicate_full]\\\n",
    "    .sort_values(['latitude', 'longitude']).head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XXko4e2zOyim"
   },
   "source": [
    "training_values.describe()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u637XYufRh8f"
   },
   "source": [
    "training_labels"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wyL52BBIFcsH"
   },
   "source": [
    "\n",
    "\n",
    "# Create the default pairplot\n",
    "sns.pairplot(training_all[columns_numerical + ['status_group']], hue = 'status_group')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPHHIaJI-tVR"
   },
   "source": [
    "# Engineer features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XscRcgEZRYuQ"
   },
   "source": [
    "# engineer some features\n",
    "\n",
    "# maybe days since reporting a functional pump?\n",
    "# lifetime: date_recorded - construction_year"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vI-9_Bsg-zaK"
   },
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['latitude', 'longitude', 'gps_height']:\n",
    "    for data in [training_values, test_values]:\n",
    "        # Impute, stepwise from specific to general (ward > lga > region > column)\n",
    "        data[column].fillna(data.groupby(['region', 'lga', 'ward'])[column].transform('mean'), inplace=True)\n",
    "        data[column].fillna(data.groupby(['region', 'lga'])[column].transform('mean'), inplace=True)\n",
    "        data[column].fillna(data.groupby(['region'])[column].transform('mean'), inplace=True)\n",
    "        data[column].fillna(data[column].mean(), inplace=True)\n",
    "display(training_values[['latitude', 'longitude', 'gps_height']].isna().sum())\n",
    "display(test_values[['latitude', 'longitude', 'gps_height']].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLzANEKyRuS9"
   },
   "source": [
    "# plot n pumps on a map. Everything above 200 gets slow\n",
    "\n",
    "n = 200\n",
    "\n",
    "gmaps.configure(api_key=\"AIzaSyCDAaxun4CXAyEmLzzJbYkqXii-sbVhVNc\")  # This is my personal API key, please don't abuse.\n",
    "\n",
    "\n",
    "\n",
    "colors = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "sampled_pumps = training_values.sample(n)\n",
    "\n",
    "for i in range(len(sampled_pumps)):\n",
    "  id = sampled_pumps.iloc[i]['id']\n",
    "  #print(id)\n",
    "  state = training_labels[training_labels['id']==id]['status_group'].iloc[0]\n",
    "  if state=='functional':\n",
    "    colors.append('green')\n",
    "  elif state=='non functional':\n",
    "    colors.append('red') \n",
    "  else:\n",
    "    colors.append('yellow') # needs repair\n",
    "\n",
    "  labels.append('source %s' % sampled_pumps[sampled_pumps['id']==id].iloc[0]['source'])\n",
    "\n",
    "\n",
    "pump_locations = sampled_pumps[['latitude' , 'longitude']]\n",
    "info_box_template = \"\"\"\n",
    "<dl>\n",
    "\n",
    "<td>Name</td><dd>{scheme_name}</dd>\n",
    "</dl>\n",
    "\"\"\"\n",
    "\n",
    "pump_info = training_values['scheme_name'][:2]\n",
    "\n",
    "#marker_layer = gmaps.marker_layer(pump_locations, hover_text=pump_info, info_box_content=pump_info)\n",
    "marker_layer = gmaps.symbol_layer(pump_locations, fill_color=colors, stroke_color=colors, scale=3, hover_text=labels)\n",
    "figure_layout = {\n",
    "    'width': '1400px',\n",
    "    'height': '1200px',\n",
    "    'border': '1px solid black',\n",
    "    'padding': '1px'\n",
    "}\n",
    "\n",
    "fig = gmaps.figure(layout=figure_layout)\n",
    "fig.add_layer(marker_layer)\n",
    "#fig\n",
    "embed_minimal_html('export.html', views=[fig])\n",
    "IPython.display.HTML(filename='export.html')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2oP1BmwhaoD9"
   },
   "source": [
    "training_values[['longitude', 'latitude']].head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFoRLkhd-5Ca"
   },
   "source": [
    "# Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rd8zedFQX7JF"
   },
   "source": [
    "\n",
    "# set n to low number for faster runs and to len(training_values) for max accuracy\n",
    "# n = 5000\n",
    "n = len(training_values)\n",
    "# select the describing variables\n",
    "# columns_select = [\n",
    "#                   'id',\n",
    "#                   'date_recorded',\n",
    "#                   'amount_tsh',\n",
    "#                   'gps_height',\n",
    "#                   'longitude',\n",
    "#                   'latitude',\n",
    "#                   'region_code',\n",
    "#                   'district_code',\n",
    "#                   'population',\n",
    "#                   'construction_year',\n",
    "#                   'source',\n",
    "#                   'quality_group',\n",
    "#                   'quantity_group',\n",
    "#                   'extraction_type_group',\n",
    "#                   ]\n",
    "columns_select = [\n",
    "                  'date_recorded',\n",
    "                  'amount_tsh',\n",
    "                  'gps_height',\n",
    "                  'longitude',\n",
    "                  'latitude',\n",
    "                  'region_code',\n",
    "                  'district_code',\n",
    "                  'population',\n",
    "                  'construction_year',\n",
    "                  'source',\n",
    "                  'source_class',\n",
    "                  'management_group',\n",
    "                  'payment_type',\n",
    "                  'extraction_type_group',\n",
    "                  'waterpoint_type_group',\n",
    "                  'quality_group',\n",
    "                  'quantity_group',\n",
    "                  'extraction_type_group',\n",
    "                 ]\n",
    "X = pd.get_dummies(training_values[columns_select][:n])\n",
    "#X = pd.get_dummies(training_values[:n])\n",
    "# X=X.drop(X[X['construction_year']< 1900].index) # drop all lines with missing construction year (but thenalso drop the y!!)\n",
    "X['lifetime']=pd.DatetimeIndex(X['date_recorded']).year-X['construction_year']  # engineer a feature but don't do it for rows where construction_year is empty\n",
    "X.loc[X['lifetime']> 1000, 'lifetime']=-1\n",
    "X['date_recorded']=X['date_recorded'].apply(datetime.toordinal) # otherwise dates get ignored in the correlation and the tree\n",
    "\n",
    "Y = pd.get_dummies(training_labels[['status_group']][:n])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "X_test_normalized  = scaler.transform(X_test)\n",
    "\n",
    "# Load and transform verification/submission data\n",
    "X_submission = pd.get_dummies(test_values[columns_select][:n])\n",
    "X_submission['lifetime']=pd.DatetimeIndex(X_submission['date_recorded']).year-X_submission['construction_year']\n",
    "X_submission.loc[X_submission['lifetime']> 1000, 'lifetime']=-1\n",
    "X_submission['date_recorded']=X_submission['date_recorded'].apply(datetime.toordinal) # otherwise dates get ignored in the correlation and the tree\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ux1_vI5d2jbe"
   },
   "source": [
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wu8gomyui65T"
   },
   "source": [
    "Y_train"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qPhexPmsgZ0-"
   },
   "source": [
    "np.array(X_train['lifetime'][:100])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KqlNFS1OUrAN"
   },
   "source": [
    "# figure out which variables correlate with Y\n",
    "\n",
    "\n",
    "sns.set(rc={'figure.facecolor':'#a0a0a0'})\n",
    "\n",
    "XY=pd.concat([X, Y], axis=1) # get them side by side\n",
    "\n",
    "corrMatrix = XY.corr()\n",
    "plt.figure(figsize=(60,25))\n",
    "# for tips on formatting the heatmap:\n",
    "# https://heartbeat.fritz.ai/seaborn-heatmaps-13-ways-to-customize-correlation-matrix-visualizations-f1c49c816f07\n",
    "sns.heatmap(corrMatrix, annot=True,  fmt='.2f', vmin=-1, vmax=1, center= 0, cmap= 'coolwarm')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnfLpQ-IRJuI"
   },
   "source": [
    "#Forecast"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "21pkGct572-T"
   },
   "source": [
    "def calc_accuracy(y_pred, Y_test):\n",
    "  correct = 0\n",
    "  for i in range(len(y_pred)):\n",
    "    y_vals = Y_test.iloc[i].values\n",
    "    y_pred_vals = y_pred[i]\n",
    "    #print(y_vals, y_pred_vals)\n",
    "    if (y_vals == y_pred_vals).all():\n",
    "      #print(\"correct\")\n",
    "      correct += 1\n",
    "    #else:\n",
    "      #print('incorrect')\n",
    "    #if correct>10: break\n",
    "  return correct/len(y_pred), correct\n",
    "\n",
    "results = {}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyNH4rTsRlO0"
   },
   "source": [
    "##Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OZDyxJP1lXAp"
   },
   "source": [
    "print(\"Train on %d samples. Test on %d samples.\" % (len(X_train), len(X_test)))\n",
    "\n",
    "results['tree'] = 0\n",
    "for d in [1, 5, 10, 15, 20, 25, 5]: # end with 5 so it can be plotted in next cell\n",
    "  model = tree.DecisionTreeClassifier(criterion='gini',max_depth=d)\n",
    "  model = model.fit(X_train, Y_train)\n",
    "\n",
    "  #Predict the response for test dataset\n",
    "  y_pred = model.predict(X_test)\n",
    " \n",
    "  accuracy, correct=calc_accuracy(y_pred, Y_test)\n",
    "  print(\"Max depth: %d   Accuracy on test set: %.2f   #correct: %d\" % (d, accuracy, correct))\n",
    "  if accuracy > results['tree']: results['tree']=accuracy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nQp0frzjlYo_"
   },
   "source": [
    "# Export/Print a decision tree in DOT format. Only do this when max_depth is small (<=6) otherwise it gets too slow.\n",
    "#print(tree.export_graphviz(clf, None))\n",
    "\n",
    "if d < 6:\n",
    "  print('extracting dot')\n",
    "  #Create Dot Data\n",
    "  dot_data = tree.export_graphviz(model, out_file=None, feature_names=list(X_train.columns.values), \n",
    "                                  class_names=['func', 'repair', 'nonfunc'], rounded=True, filled=True) #Gini decides which attribute/feature should be placed at the root node, which features will act as internal nodes or leaf nodes\n",
    "  #print(dot_data)\n",
    "  print('Create graph image from DOT data')\n",
    "  graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "  print('Show graph')\n",
    "  Image(graph.create_png())\n",
    "else:\n",
    "  print('graph to deep to fit in image')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TitZ_1pRbHIu"
   },
   "source": [
    "##Random forest"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cwKWxHMmasjX"
   },
   "source": [
    "print(\"Train on %d samples, %d variables. Test on %d samples.\" % (X_train.shape[0], X_train.shape[1], len(X_test)))\n",
    "\n",
    "d=20\n",
    "model = RandomForestClassifier(n_jobs=None,random_state=27,verbose=0, max_depth=d, criterion='gini')\n",
    "model = model.fit(X_train, Y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# accuracy, correct=calc_accuracy(y_pred, Y_test)\n",
    "# print(\"Max depth: %d   Accuracy on test set: %.2f   #correct: %d\" % (d, accuracy, correct))\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(\"Max depth: %d   Accuracy on test set: %.2f\" % (d, accuracy))\n",
    "results['forest']=accuracy\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9FnQqh1FABWE",
    "outputId": "1f001ef9-2481-489d-d94f-e8071f23bebe",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    }
   },
   "source": [
    "# feature importances\n",
    "#inspiration: https://github.com/ernestng11/touchpoint-prediction/blob/master/model-building.ipynb\n",
    "\n",
    "print(len(model.feature_importances_))\n",
    "combined = zip(model.feature_importances_, X_train.columns)\n",
    "combined = sorted(combined, reverse=True)[:50]\n",
    "#print(combined)\n",
    "#for i in len(combined):\n",
    "#  print('%s\\t%.3f' % (combined[i][1], combined[i][0]))\n",
    "\n",
    "importance, features = list(zip(*combined))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(35,5))\n",
    "plot = sns.barplot(x=np.array(features), y=np.array(importance))\n",
    "ax.set_title('Feature Importance')\n",
    "plot.set_xticklabels(plot.get_xticklabels(),rotation='vertical')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ0aE1t8RRTO"
   },
   "source": [
    "##KNN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hBkoB37GCayu",
    "outputId": "337b4e6e-c637-42c3-b1cb-4c1c962e4c04",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    }
   },
   "source": [
    "print(\"Train on %d samples. Test on %d samples.\" % (len(X_train), len(X_test)))\n",
    "\n",
    "\n",
    "results['knn']=-1\n",
    "for d in [1, 2, 3, 5, 10, 15, 20, 30]:\n",
    "  model = KNeighborsClassifier(n_neighbors=d)\n",
    "  model = model.fit(X_train_normalized, Y_train)\n",
    "\n",
    "  #Predict the response for test dataset\n",
    "  y_pred = model.predict(X_test_normalized)\n",
    "\n",
    "  accuracy, correct=calc_accuracy(y_pred, Y_test)\n",
    "  print(\"n_neighbors: %d   Accuracy on test set: %.2f   #correct: %d\" % (d, accuracy, correct))\n",
    "  if accuracy > results['knn']: results['knn']=accuracy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L-cFwS5CHzMB",
    "outputId": "7c71f624-fdb3-4ebf-fc2e-975889b2a920",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    }
   },
   "source": [
    "pd.DataFrame( Y_train)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gM_orokRXhO"
   },
   "source": [
    "##Neuralnet"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KPk1W5P-E4iv",
    "outputId": "a505866d-3612-4af7-b94b-fb537c7bca23",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "print(\"Train on %d samples. Test on %d samples.\" % (len(X_train), len(X_test)))\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential()\n",
    "#model.add(layers.Dense(2, activation=\"relu\"))\n",
    "model.add(layers.Dense(20,  activation=\"relu\", input_shape = (X_test_normalized.shape[1],)))\n",
    "model.add(layers.Dense(10,  activation=\"relu\"))\n",
    "model.add(layers.Dense(5,  activation=\"relu\"))\n",
    "model.add(layers.Dense(3,   activation='sigmoid'))\n",
    "model.compile('adam', \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x=X_train_normalized, y=Y_train, epochs=35)\n",
    "model.summary()\n",
    "\n",
    "y_pred = model.predict(X_test_normalized)\n",
    "print(len(y_pred))\n",
    "y_pred = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "accuracy, correct=calc_accuracy(y_pred, Y_test)\n",
    "print(\"Accuracy on test set: %.2f   #correct: %d\" % (accuracy, correct))\n",
    "results['neural net']=accuracy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSrcI0Gae9cE"
   },
   "source": [
    "##XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YK24eHPVfFQw",
    "outputId": "22dba33c-d6ce-49c8-a341-556ae4bbb141",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "source": [
    "# inspiration: https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "#for d in range(1,35):\n",
    "results['xgboost']=-1\n",
    "#for d in [2, 15, 30, 50]:\n",
    "for d in [30]:\n",
    "  model = OneVsRestClassifier(XGBClassifier(n_jobs=-1, max_depth=d, objective=\"multi:softprob\", num_class=3))\n",
    "  model = model.fit(X_train_normalized, Y_train)\n",
    "\n",
    "  #Predict the response for test dataset\n",
    "  y_pred = model.predict(X_test_normalized)\n",
    "\n",
    "  accuracy, correct=calc_accuracy(y_pred, Y_test)\n",
    "  print(\"XGBoost: max_depth: %d   Accuracy on test set: %.2f   #correct: %d\" % (d, accuracy, correct))\n",
    "  if accuracy>results['xgboost']: results['xgboost']=accuracy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wtt6Yw9AZMoc"
   },
   "source": [
    "#print(confusion_matrix(Y_test, y_pred))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKXg_c92EjZZ"
   },
   "source": [
    "#Evaluation\n",
    "- randomforest: .72 \n",
    "- tree: .70\n",
    "- xgboost: .70\n",
    "- nn: .65\n",
    "- knn: .48"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eiN5HwCD9cEC"
   },
   "source": [
    "for k in results.keys():\n",
    "  print('%s: %.2f' % (k.capitalize(), results[k]))\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q6LPzRASj4M5"
   },
   "source": [
    "import requests\n",
    "gcloud_token = !gcloud auth print-access-token\n",
    "gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
    "gcloud_tokeninfo\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHrNq7Z1PuQt"
   },
   "source": [
    "#Submit result"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G0ag_iZe-aE3"
   },
   "source": [
    "print('train model')\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=None,random_state=27,verbose=0, max_depth=20, criterion='gini')\n",
    "# re-train on the full training set\n",
    "model = model.fit(X, Y)\n",
    "\n",
    "print('predict')\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(X_submission)\n",
    "\n",
    "print('create submission')\n",
    "# create a dataframe for submission\n",
    "# TODO: For better performance write this without a loop with a zip() or map()\n",
    "submission = pd.DataFrame(columns=['id', 'status_group'])\n",
    "for i in range(len(y_pred)):\n",
    "  if y_pred[i][0]: status='functional'\n",
    "  if y_pred[i][1]: status='functional needs repair'\n",
    "  if y_pred[i][2]: status='non functional'\n",
    "  submission=submission.append({'id': test_values.iloc[i]['id'], 'status_group': status}, ignore_index=True)\n",
    "\n",
    "# save as csv\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-Qv6DQ-SS5Gi"
   },
   "source": [
    "test_values"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5Vdfr1-xWIJD"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww0xXVHe201e"
   },
   "source": [
    "# Graveyard\n",
    "Snippets that are incomplete but interesting nonetheless"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KwCo_6lJCtV1"
   },
   "source": [
    "# inspired by: https://medium.com/@gabrielziegler3/multiclass-multilabel-classification-with-xgboost-66195e4d9f2d\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "model = XGBClassifier(max_depth=5, objective='multi:softprob', n_estimators=1000, \n",
    "                        num_classes=3)\n",
    "model = model.fit(X_train_normalized, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = model.predict(X_test_normalized)\n",
    "\n",
    "accuracy=calc_accuracy(y_pred, Y_test)\n",
    "print(\"n_neighbors: %d   Accuracy on test set: %.2f   #correct: %d\" % (d, accuracy, correct))\n",
    "accuracy_xgboost=accuracy"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}